# main.py
import os
import time
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory

from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage

import functions as ft
import constants as ct


# ==============================
# åˆæœŸè¨­å®š
# ==============================
load_dotenv()

# å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆãªã‘ã‚Œã°ä½œã‚‹ï¼‰
os.makedirs(ct.AUDIO_INPUT_DIR, exist_ok=True)
os.makedirs(ct.AUDIO_OUTPUT_DIR, exist_ok=True)

st.set_page_config(page_title=ct.APP_NAME)
st.markdown(f"## {ct.APP_NAME}")

# ==============================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
# ==============================
if "initialized" not in st.session_state:
    st.session_state.initialized = True

    # UI / çŠ¶æ…‹
    st.session_state.messages = []
    st.session_state.start_flg = False
    st.session_state.pre_mode = ""
    st.session_state.speed = 1.0
    st.session_state.intro_shown = False

    # ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°
    st.session_state.shadowing_flg = False
    st.session_state.shadowing_button_flg = False
    st.session_state.shadowing_count = 0
    st.session_state.shadowing_first_flg = True
    st.session_state.shadowing_audio_input_flg = False
    st.session_state.shadowing_evaluation_first_flg = True
    st.session_state.shadowing_in_progress = False
    st.session_state.shadowing_eval_target = ""

    # ãƒ‡ã‚£ã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
    st.session_state.dictation_flg = False
    st.session_state.dictation_button_flg = False
    st.session_state.dictation_count = 0
    st.session_state.dictation_first_flg = True
    st.session_state.dictation_chat_message = ""
    st.session_state.dictation_evaluation_first_flg = True

    # å…±é€š
    st.session_state.chat_open_flg = False
    st.session_state.problem = ""
    st.session_state.englv = ct.ENGLISH_LEVEL_OPTION[1]  # æ—¢å®š: ä¸­ç´šè€…

    # OpenAI API ã‚­ãƒ¼
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # OpenAI & LangChain
    st.session_state.openai_obj = OpenAI(api_key=api_key)
    st.session_state.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
    st.session_state.memory = ConversationBufferMemory(
        return_messages=True,
        memory_key="history",  # MessagesPlaceholder("history") ã¨ä¸€è‡´
    )

    # æ—¥å¸¸è‹±ä¼šè©±ãƒã‚§ãƒ¼ãƒ³
    st.session_state.chain_basic_conversation = ft.create_chain(
        ct.SYSTEM_TEMPLATE_BASIC_CONVERSATION
    )

# ==============================
# ç”»é¢ä¸Šéƒ¨ UI
# ==============================
col1, col2, col3, col4 = st.columns([2, 2, 3, 3])

with col1:
    clicked_start = st.button("é–‹å§‹", use_container_width=True, type="primary")
    if clicked_start:
        st.session_state.start_flg = True

with col2:
    st.session_state.speed = st.selectbox(
        label="å†ç”Ÿé€Ÿåº¦",
        options=ct.PLAY_SPEED_OPTION,
        index=3,
        label_visibility="collapsed"
    )

with col3:
    st.session_state.mode = st.selectbox(
        label="ãƒ¢ãƒ¼ãƒ‰",
        options=[ct.MODE_1, ct.MODE_2, ct.MODE_3],
        label_visibility="collapsed"
    )
    # ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿æ™‚ã®åˆæœŸåŒ–
    if st.session_state.mode != st.session_state.pre_mode:
        st.session_state.start_flg = False
        if st.session_state.mode == ct.MODE_1:
            st.session_state.dictation_flg = False
            st.session_state.shadowing_flg = False
        st.session_state.shadowing_count = 0
        if st.session_state.mode == ct.MODE_2:
            st.session_state.dictation_flg = False
        st.session_state.dictation_count = 0
        if st.session_state.mode == ct.MODE_3:
            st.session_state.shadowing_flg = False
        st.session_state.chat_open_flg = False
        # ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°é€²è¡Œãƒ•ãƒ©ã‚°ã¯å¸¸ã«è½ã¨ã™
        st.session_state.shadowing_in_progress = False
    st.session_state.pre_mode = st.session_state.mode

with col4:
    st.session_state.englv = st.selectbox(
        label="è‹±èªãƒ¬ãƒ™ãƒ«",
        options=ct.ENGLISH_LEVEL_OPTION,
        index=ct.ENGLISH_LEVEL_OPTION.index(st.session_state.get("englv", "ä¸­ç´šè€…")),
        label_visibility="collapsed"
    )

# åˆå›ã®ã¿æ“ä½œèª¬æ˜ã‚’è¡¨ç¤º
if not st.session_state.intro_shown:
    with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
        st.markdown("ã“ã¡ã‚‰ã¯ç”ŸæˆAIã«ã‚ˆã‚‹éŸ³å£°è‹±ä¼šè©±ã®ç·´ç¿’ã‚¢ãƒ—ãƒªã§ã™ã€‚ä½•åº¦ã‚‚ç¹°ã‚Šè¿”ã—ç·´ç¿’ã—ã€è‹±èªåŠ›ã‚’ã‚¢ãƒƒãƒ—ã•ã›ã¾ã—ã‚‡ã†ã€‚")
        st.markdown("**ã€æ“ä½œèª¬æ˜ã€‘**")
        st.success("""
- ãƒ¢ãƒ¼ãƒ‰ã¨å†ç”Ÿé€Ÿåº¦ã‚’é¸æŠã—ã€ã€Œé–‹å§‹ã€ãƒœã‚¿ãƒ³ã§ç·´ç¿’ã‚’å§‹ã‚ã¾ã™ã€‚
- ãƒ¢ãƒ¼ãƒ‰ã¯ã€Œæ—¥å¸¸è‹±ä¼šè©±ã€ã€Œã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°ã€ã€Œãƒ‡ã‚£ã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‹ã‚‰é¸ã¹ã¾ã™ã€‚
- ç™ºè©±å¾Œã€5ç§’é–“æ²ˆé»™ã™ã‚‹ã“ã¨ã§éŸ³å£°å…¥åŠ›ãŒå®Œäº†ã—ã¾ã™ã€‚
- ã€Œä¸€æ™‚ä¸­æ–­ã€ãƒœã‚¿ãƒ³ç›¸å½“ã®æŒ™å‹•ã¯ã€é–‹å§‹ãƒœã‚¿ãƒ³ã‚’å†æŠ¼ä¸‹ã—ãªã„ã“ã¨ã§ä»£æ›¿ã§ãã¾ã™ã€‚
""")
    st.session_state.intro_shown = True
st.divider()

# ==============================
# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®è¡¨ç¤º
# ==============================
for message in st.session_state.messages:
    if message["role"] == "assistant":
        with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
            st.markdown(message["content"])
    elif message["role"] == "user":
        with st.chat_message("user", avatar=ct.USER_ICON_PATH):
            st.markdown(message["content"])
    else:
        st.divider()

# å®Ÿè¡Œè£œåŠ©ãƒœã‚¿ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
if st.session_state.shadowing_flg:
    st.session_state.shadowing_button_flg = st.button("ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°é–‹å§‹")
if st.session_state.dictation_flg:
    st.session_state.dictation_button_flg = st.button("ãƒ‡ã‚£ã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")

# ãƒ‡ã‚£ã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å…¥åŠ›æ¡ˆå†…
if st.session_state.chat_open_flg:
    st.info("AIãŒèª­ã¿ä¸Šã’ãŸéŸ³å£°ã‚’ã€ç”»é¢ä¸‹éƒ¨ã®ãƒãƒ£ãƒƒãƒˆæ¬„ã‹ã‚‰ãã®ã¾ã¾å…¥åŠ›ãƒ»é€ä¿¡ã—ã¦ãã ã•ã„ã€‚")

st.session_state.dictation_chat_message = st.chat_input("â€»ã€Œãƒ‡ã‚£ã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€é¸æŠæ™‚ä»¥å¤–ã¯é€ä¿¡ä¸å¯")
if st.session_state.dictation_chat_message and not st.session_state.chat_open_flg:
    st.stop()

# ==============================
# ã€Œé–‹å§‹ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹å¾Œã®å‡¦ç†
# ==============================
if st.session_state.start_flg:

    # ---------- ãƒ¢ãƒ¼ãƒ‰ï¼šãƒ‡ã‚£ã‚¯ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ----------
    if st.session_state.mode == ct.MODE_3 and (
        st.session_state.dictation_button_flg
        or st.session_state.dictation_count == 0
        or st.session_state.dictation_chat_message
    ):
        if st.session_state.dictation_first_flg:
            st.session_state.chain_create_problem = ft.create_chain(ct.SYSTEM_TEMPLATE_CREATE_PROBLEM)
            st.session_state.dictation_first_flg = False

        # ã¾ã å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã¦ã„ãªã„ï¼å•é¡Œæ–‡ã®æç¤ºãƒ•ã‚§ãƒ¼ã‚º
        if not st.session_state.chat_open_flg:
            with st.spinner('å•é¡Œæ–‡ç”Ÿæˆä¸­...'):
                st.session_state.problem, _ = ft.create_problem_and_play_audio()

            # ç”Ÿæˆã—ãŸè‹±æ–‡ã‚’å³è¡¨ç¤ºï¼ˆå¯è¦–åŒ–ï¼‰
            with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                st.markdown(st.session_state.problem)
            st.session_state.messages.append(
                {"role": "assistant", "content": st.session_state.problem}
            )

            # å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹çŠ¶æ…‹ã«é·ç§»ï¼ˆrerunã¯è¡Œã‚ãšã€ã“ã“ã§ä¸€æ—¦åœæ­¢ã—ã¦å…¥åŠ›å¾…ã¡ï¼‰
            st.session_state.chat_open_flg = True
            st.session_state.dictation_flg = False
            st.stop()

        # å›ç­”å…¥åŠ›å¾Œã®è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º
        else:
            if not st.session_state.dictation_chat_message:
                st.stop()

            with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                st.markdown(st.session_state.problem)
            with st.chat_message("user", avatar=ct.USER_ICON_PATH):
                st.markdown(st.session_state.dictation_chat_message)

            st.session_state.messages.append({"role": "assistant", "content": st.session_state.problem})
            st.session_state.messages.append({"role": "user", "content": st.session_state.dictation_chat_message})

            with st.spinner('è©•ä¾¡çµæœã®ç”Ÿæˆä¸­...'):
                system_template = ct.SYSTEM_TEMPLATE_EVALUATION.format(
                    llm_text=st.session_state.problem,
                    user_text=st.session_state.dictation_chat_message
                )
                st.session_state.chain_evaluation = ft.create_chain(system_template)
                llm_response_evaluation = ft.create_evaluation()

            with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                st.markdown(llm_response_evaluation)
            st.session_state.messages.append({"role": "assistant", "content": llm_response_evaluation})
            st.session_state.messages.append({"role": "other"})

            st.session_state.dictation_flg = True
            st.session_state.dictation_chat_message = ""
            st.session_state.dictation_count += 1
            st.session_state.chat_open_flg = False
            st.rerun()

    # ---------- ãƒ¢ãƒ¼ãƒ‰ï¼šæ—¥å¸¸è‹±ä¼šè©± ----------
    if st.session_state.mode == ct.MODE_1:
        audio_input_file_path = f"{ct.AUDIO_INPUT_DIR}/audio_input_{int(time.time())}.wav"
        audio_input_file_path = audio_input_file_path.replace("}", "")  # ä¸‡ä¸€ã®æ³¢æ‹¬å¼§æ··å…¥å›é¿
        ft.record_audio(audio_input_file_path)

        with st.spinner('éŸ³å£°å…¥åŠ›ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ä¸­...'):
            transcript = ft.transcribe_audio(audio_input_file_path)
            audio_input_text = transcript.text

        with st.chat_message("user", avatar=ct.USER_ICON_PATH):
            st.markdown(audio_input_text)

        with st.spinner("å›ç­”ã®éŸ³å£°èª­ã¿ä¸Šã’æº–å‚™ä¸­..."):
            llm_response = st.session_state.chain_basic_conversation.predict(input=audio_input_text)
            resp = st.session_state.openai_obj.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input=llm_response
            )
            audio_bytes = getattr(resp, "content", resp)
            audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
            ft.save_to_wav(audio_bytes, audio_output_file_path)

        ft.play_wav(audio_output_file_path, speed=st.session_state.speed)

        with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
            st.markdown(llm_response)

        st.session_state.messages.append({"role": "user", "content": audio_input_text})
        st.session_state.messages.append({"role": "assistant", "content": llm_response})

    # ---------- ãƒ¢ãƒ¼ãƒ‰ï¼šã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚° ----------
    if st.session_state.mode == ct.MODE_2 and (
        st.session_state.shadowing_button_flg
        or st.session_state.get("shadowing_in_progress", False)
        or st.session_state.shadowing_count == 0
    ):
        # åˆå›ãƒã‚§ãƒ¼ãƒ³
        if st.session_state.shadowing_first_flg:
            st.session_state.chain_create_problem = ft.create_chain(ct.SYSTEM_TEMPLATE_CREATE_PROBLEM)
            st.session_state.shadowing_first_flg = False

        # é€²è¡Œä¸­ã§ãªã‘ã‚Œã°ã€Œå•é¡Œæ±ºå®šâ†’TTSå†ç”Ÿâ†’éŒ²éŸ³ã¸ã€
        if not st.session_state.shadowing_in_progress:
            custom_sentence = st.text_input(
                "èª­ã¿ä¸Šã’ãŸã„è‹±æ–‡ã‚’å…¥åŠ›ï¼ˆç©ºæ¬„ãªã‚‰AIãŒè‡ªå‹•ç”Ÿæˆã—ã¾ã™ï¼‰",
                placeholder="ä¾‹ï¼šIt's a beautiful day today, isn't it?"
            )

            if "show_text_flg" not in st.session_state:
                st.session_state.show_text_flg = True
            st.session_state.show_text_flg = st.checkbox(
                "è‹±æ–‡ã‚’è¡¨ç¤ºã™ã‚‹ï¼ˆONã§è¦‹ãªãŒã‚‰ç·´ç¿’ï¼‰",
                value=st.session_state.show_text_flg,
                key="show_text_checkbox"
            )

            if custom_sentence:
                st.session_state.problem = custom_sentence
            else:
                with st.spinner('å•é¡Œæ–‡ç”Ÿæˆä¸­...'):
                    # â˜… ä¸‰é‡å†ç”Ÿè§£æ¶ˆï¼šã“ã“ã§ã¯å†ç”Ÿã—ãªã„ç‰ˆã‚’ä½¿ç”¨
                    st.session_state.problem = ft.generate_problem_only()

            # è©•ä¾¡å¯¾è±¡ã‚’å›ºå®š
            st.session_state.shadowing_eval_target = st.session_state.problem

            # è¡¨ç¤ºï¼ˆå±¥æ­´ã¯é‡è¤‡ç™»éŒ²å›é¿ï¼‰
            if st.session_state.show_text_flg:
                with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                    st.markdown(f"**ç·´ç¿’æ–‡:** {st.session_state.problem}")
            if not st.session_state.messages or st.session_state.messages[-1].get("content") != f"**ç·´ç¿’æ–‡:** {st.session_state.problem}":
                st.session_state.messages.append({"role": "assistant", "content": f"**ç·´ç¿’æ–‡:** {st.session_state.problem}"})

            # TTS 2å›å†ç”Ÿï¼ˆèãå–ã‚Šâ†’ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°ï¼‰
            resp1 = st.session_state.openai_obj.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input=st.session_state.problem
            )
            audio_bytes1 = getattr(resp1, "content", resp1)
            audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
            ft.save_to_wav(audio_bytes1, audio_output_file_path)

            st.info("ğŸ”Šã€1å›ç›®ã€‘èãå–ã‚Šç·´ç¿’ä¸­...")
            ft.play_wav(audio_output_file_path, st.session_state.speed, keep_file=True)
            st.info("ğŸ—£ï¸ã€2å›ç›®ã€‘ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°ç·´ç¿’ã‚¹ã‚¿ãƒ¼ãƒˆï¼")
            ft.play_wav(audio_output_file_path, st.session_state.speed, keep_file=False)

            # éŒ²éŸ³ãƒ•ã‚§ãƒ¼ã‚ºã¸
            st.session_state.shadowing_in_progress = True

        # éŒ²éŸ³â†’æ–‡å­—èµ·ã“ã—ï¼ˆé€²è¡Œä¸­ï¼‰
        if st.session_state.shadowing_in_progress:
            audio_input_file_path = f"{ct.AUDIO_INPUT_DIR}/audio_input_{int(time.time())}.wav"
            audio_input_file_path = audio_input_file_path.replace("}", "")
            ft.record_audio(audio_input_file_path)

            with st.spinner('éŸ³å£°å…¥åŠ›ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ä¸­...'):
                transcript = ft.transcribe_audio(audio_input_file_path)
                audio_input_text = transcript.text

            if not audio_input_text.strip():
                st.warning("éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                st.stop()

            with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                st.markdown(st.session_state.shadowing_eval_target)
            with st.chat_message("user", avatar=ct.USER_ICON_PATH):
                st.markdown(audio_input_text)

            st.session_state.messages.append({"role": "user", "content": audio_input_text})

            with st.spinner('è©•ä¾¡çµæœã®ç”Ÿæˆä¸­...'):
                system_template = ct.SYSTEM_TEMPLATE_EVALUATION.format(
                    llm_text=st.session_state.shadowing_eval_target,
                    user_text=audio_input_text
                )
                st.session_state.chain_evaluation = ft.create_chain(system_template)
                llm_response_evaluation = ft.create_evaluation()

            with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                st.markdown(llm_response_evaluation)

            st.session_state.messages.append({"role": "assistant", "content": llm_response_evaluation})
            st.session_state.messages.append({"role": "other"})

            # å¾Œç‰‡ä»˜ã‘ï¼šæ¬¡ã®å•é¡Œã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå†åº¦ã€Œé–‹å§‹ã€æŠ¼ä¸‹ or ãƒœã‚¿ãƒ³æŠ¼ä¸‹ã§
            st.session_state.shadowing_eval_target = ""
            st.session_state.shadowing_in_progress = False
            st.session_state.shadowing_flg = True
            st.session_state.shadowing_count += 1

            st.rerun()
